# -*- coding: utf-8 -*-
"""correlated-ca-counties-severe-vs-nonsevere.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vTrMKvM1uyYSFDrUPn_-5u6I6OX8f2vF
"""

import os
import pandas as pd
import numpy as np
import seaborn as sns
import pickle

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict
from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay, precision_recall_curve, PrecisionRecallDisplay

import warnings
warnings.filterwarnings('ignore')

filename = os.path.join(os.getcwd(), 'merged_weather_outages_2019_2024_encoded.csv')
df = pd.read_csv(filename)

df.head()

df_san_diego = df[df['fips_code'] == 6073].copy()
df_san_francisco = df[df['fips_code'] == 6075].copy()
print(df_san_diego.shape)
print(df_san_francisco.shape)

# remove columns with only one unique value; will not be helpful as features
san_diego_cols_removed = []

for i in df_san_diego.columns:
  if df_san_diego[i].unique().size == 1:
    print(i + ": ")
    print(df_san_diego[i].unique())
    san_diego_cols_removed.append(i)
    df_san_diego.drop(columns = [i], inplace = True)

san_francisco_cols_removed = []

for i in df_san_francisco.columns:
  if df_san_francisco[i].unique().size == 1:
    print(i + ": ")
    print(df_san_francisco[i].unique())
    san_francisco_cols_removed.append(i)
    df_san_francisco.drop(columns = [i], inplace = True)

san_diego_cols_removed == san_francisco_cols_removed

# check for non-numerical data
print(list(df_san_diego.dtypes.unique()))
print(list(df_san_francisco.dtypes.unique()))

# convert bool values to 0/1
san_diego_bool = df_san_diego.select_dtypes(include = 'bool').columns
san_francisco_bool = df_san_francisco.select_dtypes(include = 'bool').columns

for i in san_diego_bool:
  df_san_diego[i] = df_san_diego[i].astype(int)

for i in san_francisco_bool:
  df_san_francisco[i] = df_san_francisco[i].astype(int)

print(df_san_diego.shape)
df_san_diego.columns

print(df_san_francisco.shape)
df_san_francisco.columns

# top 5 highest values for pct_out_area_unified (average fraction of customers out over the entire local day)
df_san_diego['pct_out_area_unified'].nlargest()

# 5 smallest values for pct_out_area_unified
df_san_francisco['pct_out_area_unified'].nlargest()

df_san_diego['severe_out'] = (df_san_diego['pct_out_area_unified'] >= df_san_diego['pct_out_area_unified'].quantile(0.986))
df_san_francisco['severe_out'] = (df_san_francisco['pct_out_area_unified'] >= df_san_francisco['pct_out_area_unified'].quantile(0.976))

df_san_diego['severe_out'].value_counts()

df_san_francisco['severe_out'].value_counts()

# remove outage related features to prevent leakage (label will be severe_out)
df_san_diego.drop(columns = ['any_out', 'num_out_per_day', 'minutes_out', 'customers_out', 'customers_out_mean', 'cust_minute_area', 'pct_out_max', 'pct_out_area', 'snapshots_count', 'minutes_observed', 'coverage', 'pct_out_area_unified', 'pct_out_area_covered', 'pct_out_max_unified', 'train_mask'], inplace = True)
df_san_francisco.drop(columns = ['any_out', 'num_out_per_day', 'minutes_out', 'customers_out', 'customers_out_mean', 'cust_minute_area', 'pct_out_max', 'pct_out_area', 'snapshots_count', 'minutes_observed', 'coverage', 'pct_out_area_unified', 'pct_out_area_covered', 'pct_out_max_unified', 'train_mask'], inplace = True)

# SAN DIEGO: top 15 correlated features with severe_out
sd_features = df_san_diego.corr()['severe_out'].sort_values(ascending = False).iloc[1:16]
sd_features

# SAN FRANCISCO: top 6 correlated features with severe_out
sf_features = df_san_francisco.corr()['severe_out'].sort_values(ascending = False).iloc[1:7]
sf_features

# use correlated features, reserve 2022 data as test data
X_test_sd = df_san_diego[sd_features.index][df_san_diego['year'] == 2022]
y_test_sd = df_san_diego['severe_out'][df_san_diego['year'] == 2022]
X_test_sf = df_san_francisco[sf_features.index][df_san_francisco['year'] == 2022]
y_test_sf = df_san_francisco['severe_out'][df_san_francisco['year'] == 2022]

X_train_sd = df_san_diego[sd_features.index][df_san_diego['year'] != 2022]
y_train_sd = df_san_diego['severe_out'][df_san_diego['year'] != 2022]
X_train_sf = df_san_francisco[sf_features.index][df_san_francisco['year'] != 2022]
y_train_sf = df_san_francisco['severe_out'][df_san_francisco['year'] != 2022]

print("San Diego:")
print("X train shape: " + str(X_train_sd.shape))
print("X test shape: " + str(X_test_sd.shape))

print("\nSan Francisco:")
print("X train shape: " + str(X_train_sf.shape))
print("X test shape: " + str(X_test_sf.shape))

y_train_sd.value_counts()

y_test_sd.value_counts()

y_train_sf.value_counts()

y_test_sf.value_counts()

# parameter grid for grid search
solver = ['lbfgs', 'liblinear', 'saga']
penalty = ['l1', 'l2']
c = [10**i for i in range(-5,5)]
max_iter = [100*i for i in range(1, 10)]

param_grid = {'solver':solver, 'penalty':penalty, 'C':c, 'max_iter':max_iter}

# SAN DIEGO TRAINING
sd_model = LogisticRegression(class_weight = 'balanced')
skf_sd = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
# choose model with best precision-recall auc (average_precision)
sd_grid = GridSearchCV(sd_model, param_grid, cv = skf_sd, n_jobs = -1, scoring = ['accuracy', 'average_precision', 'roc_auc','precision','recall','f1'], refit = 'average_precision')
sd_grid_search = sd_grid.fit(X_train_sd, y_train_sd)

print("Best parameters: ", sd_grid_search.best_params_)
for metric in ['accuracy', 'average_precision', 'roc_auc', 'precision', 'recall', 'f1']:
    value = sd_grid_search.cv_results_[f'mean_test_{metric}'][sd_grid_search.best_index_]
    print(f"{metric}: {value}")

# SAN DIEGO TESTING
sd_best_model = sd_grid_search.best_estimator_
sd_y_pred = sd_best_model.predict(X_test_sd)

print("accuracy: " + str(accuracy_score(y_test_sd, sd_y_pred)))
print("average precision: " + str(average_precision_score(y_test_sd, sd_y_pred)))
print("roc auc: " + str(roc_auc_score(y_test_sd, sd_y_pred)))
print("precision: " + str(precision_score(y_test_sd, sd_y_pred)))
print("recall: " + str(recall_score(y_test_sd, sd_y_pred)))
print("f1: " + str(f1_score(y_test_sd, sd_y_pred)))

c_m = confusion_matrix(y_test_sd, sd_y_pred, labels = [1, 0])
sd_cm = ConfusionMatrixDisplay(c_m, display_labels = ['Severe', 'Non-Severe']).plot(cmap = 'Blues')
sd_cm.ax_.set_yticklabels(sd_cm.ax_.get_yticklabels(), rotation = 90)

# SAN FRANCISCO TRAINING
sf_model = LogisticRegression(class_weight = 'balanced')
skf_sf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)
sf_grid = GridSearchCV(sf_model, param_grid, cv = skf_sf, n_jobs = -1, scoring = ['accuracy', 'average_precision', 'roc_auc','precision','recall','f1'], refit = 'average_precision')
sf_grid_search = sf_grid.fit(X_train_sf, y_train_sf)

print("Best parameters: ", sf_grid_search.best_params_)
for metric in ['accuracy', 'average_precision', 'roc_auc', 'precision', 'recall', 'f1']:
    value = sf_grid_search.cv_results_[f'mean_test_{metric}'][sf_grid_search.best_index_]
    print(f"{metric}: {value}")

# SAN FRANCISCO TESTING
sf_best_model = sf_grid_search.best_estimator_
sf_y_pred = sf_best_model.predict(X_test_sf)

print("accuracy: " + str(accuracy_score(y_test_sf, sf_y_pred)))
print("average precision: " + str(average_precision_score(y_test_sf, sf_y_pred)))
print("roc auc: " + str(roc_auc_score(y_test_sf, sf_y_pred)))
print("precision: " + str(precision_score(y_test_sf, sf_y_pred)))
print("recall: " + str(recall_score(y_test_sf, sf_y_pred)))
print("f1: " + str(f1_score(y_test_sf, sf_y_pred)))

c_m = confusion_matrix(y_test_sf, sf_y_pred, labels = [1, 0])
sf_cm = ConfusionMatrixDisplay(c_m, display_labels = ['Severe', 'Non-Severe']).plot(cmap = 'Greens')
sf_cm.ax_.set_yticklabels(sf_cm.ax_.get_yticklabels(), rotation = 90)

pickle.dump(sd_best_model, open('san_diego_model.pkl', 'wb'))
pickle.dump(sf_best_model, open('san_francisco_model.pkl', 'wb'))